{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3139829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "training_file = '%_nombank.clean.train'\n",
    "test_file = '%_nombank.clean.test'\n",
    "file = open(training_file,\"r\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "f = open(\"training.feature\", \"w\")\n",
    "training_features = []\n",
    "for line in lines:\n",
    "    t = line.split()\n",
    "    training_features.append(t)\n",
    "\n",
    "\n",
    "sentence = []\n",
    "# text_lines = []\n",
    "# arg1_index = 0\n",
    "for i in range(len(training_features)):\n",
    "    t = training_features[i]\n",
    "    if len(t) == 0:\n",
    "        \n",
    "\n",
    "\n",
    "        line = \"\\n\"\n",
    "        f.write(line)\n",
    "        sentence.clear()\n",
    "#         text_lines.clear()\n",
    "        # print(\"end\")\n",
    "        continue\n",
    "    token = t[0]\n",
    "    POS_tag = t[1]\n",
    "    BIO_tag = t[2]\n",
    "#     token_num = t[3]\n",
    "#     sentence_num = t[4]\n",
    "#     role_label = \"\"\n",
    "#     class_of_predicate = \"\"\n",
    "    arg1_label = 0\n",
    "    stemmed_token = ps.stem(token)\n",
    "    if len(t) > 5:\n",
    "#         role_label = t[5] \n",
    "#         if t[5] == \"PRED\":\n",
    "#             class_of_predicate = t[6]\n",
    "        if t[5] == \"ARG1\":\n",
    "            arg1_label = 1\n",
    "    prev_info1 = \"\"\n",
    "    prev_info2 = \"\"\n",
    "    if training_features[i-1]:\n",
    "        prev_t1 = training_features[i-1]\n",
    "        prev_token1 = prev_t1[0]\n",
    "        prev_POS_tag1 = prev_t1[1]\n",
    "        prev_BIO_tag1 = prev_t1[2]\n",
    "#         prev_token_num1 = prev_t1[3]\n",
    "#         prev_sentence_num1 = prev_t1[4]\n",
    "        stemmed_prev_token1 = ps.stem(prev_token1)\n",
    "#         prev_role_label1 = \"\"\n",
    "#         prev_class_of_predicate1 = \"\"\n",
    "#         if len(prev_t1) > 5:\n",
    "#             prev_role_label1 = prev_t1[5] + \"\\t\"\n",
    "#             if prev_t1[5] == \"PRED\":\n",
    "#                 prev_class_of_predicate1 = prev_t1[6] + \"\\t\"\n",
    "\n",
    "        prev_info1 = \"word_back_1=\" + prev_token1 + \"\\tPOS_back_1=\" + prev_POS_tag1 + \"\\tBIO_back_1=\" + prev_BIO_tag1 + \"\\tstemmed_word_back_1=\" \\\n",
    "                        + stemmed_prev_token1 + \"\\t\" \n",
    "#                 + prev_token_num1 + \"\\t\" + prev_sentence_num1 + \"\\t\" + prev_role_label1 + prev_class_of_predicate1\n",
    "                            \n",
    "\n",
    "        if training_features[i-2]:\n",
    "            prev_t2 = training_features[i-2]\n",
    "            prev_token2 = prev_t2[0]\n",
    "            prev_POS_tag2 = prev_t2[1]\n",
    "            prev_BIO_tag2 = prev_t2[2]\n",
    "#             prev_token_num2 = prev_t2[3]\n",
    "#             prev_sentence_num2 = prev_t2[4]\n",
    "            stemmed_prev_token2 = ps.stem(prev_token2)\n",
    "#             prev_role_label2 = \"\"\n",
    "#             prev_class_of_predicate2 = \"\"\n",
    "#             if len(prev_t2) > 5:\n",
    "#                 prev_role_label2 = prev_t2[5] + \"\\t\"\n",
    "#                 if prev_t2[5] == \"PRED\":\n",
    "#                     prev_class_of_predicate2 = prev_t2[6] + \"\\t\"\n",
    "\n",
    "            prev_info2 = \"word_back_2=\" + prev_token2 + \"\\tPOS_back_2=\" + prev_POS_tag2 + \"\\tBIO_back_2=\" + prev_BIO_tag2 + \"\\tstemmed_word_back_2=\" \\\n",
    "                        + stemmed_prev_token2 + \"\\t\" \n",
    "#                     + prev_token_num2 + \"\\t\" + prev_sentence_num2 + \"\\t\" + prev_role_label2 + prev_class_of_predicate2 \n",
    "                        \n",
    "    \n",
    "    else:\n",
    "        prev_info1 = \"Beginning Sentence\\t\"\n",
    "\n",
    "    next_info1 = \"\"\n",
    "    next_info2 = \"\"\n",
    "    if training_features[i+1]:\n",
    "        next_t1 = training_features[i+1]\n",
    "        next_token1 = next_t1[0]\n",
    "        next_POS_tag1 = next_t1[1]\n",
    "        next_BIO_tag1 = next_t1[2]\n",
    "        stemmed_next_token1 = ps.stem(next_token1)\n",
    "#         next_token_num1 = next_t1[3]\n",
    "#         next_sentence_num1 = next_t1[4]\n",
    "#         next_role_label1 = \"\"\n",
    "#         next_class_of_predicate1 = \"\"\n",
    "#         if len(next_t1) > 5:\n",
    "#                 next_role_label1 = next_t1[5] + \"\\t\"\n",
    "#                 if next_t1[5] == \"PRED\":\n",
    "#                     next_class_of_predicate1 = next_t1[6] + \"\\t\"\n",
    "        next_info1 = \"word_plus_1=\" + next_token1 + \"\\tPOS_plus_1=\" + next_POS_tag1 + \"\\tBIO_plus_1=\" + next_BIO_tag1 + \"\\tstemmed_word_plus_1=\" \\\n",
    "                            + stemmed_next_token1 + \"\\t\" \n",
    "#                     + next_token_num1 + \"\\t\" + next_sentence_num1 + \"\\t\" + next_role_label1 + next_class_of_predicate1\n",
    "                        \n",
    "\n",
    "        if training_features[i+2]:\n",
    "            next_t2 = training_features[i+2]\n",
    "            next_token2 = next_t2[0]\n",
    "            next_POS_tag2 = next_t2[1]\n",
    "            next_BIO_tag2 = next_t2[2]\n",
    "            stemmed_next_token2 = ps.stem(next_token2)\n",
    "#             next_token_num2 = next_t2[3]\n",
    "#             next_sentence_num2 = next_t2[4]\n",
    "            next_role_label2 = \"\"\n",
    "            next_class_of_predicate2 = \"\"\n",
    "            if len(next_t2) > 5:\n",
    "                    next_role_label2 = next_t2[5] + \"\\t\"\n",
    "                    if next_t2[5] == \"PRED\":\n",
    "                        next_class_of_predicate2 = next_t2[6] + \"\\t\"\n",
    "            next_info2 = \"word_plus_2=\" + next_token2 + \"\\tPOS_plus_2=\" + next_POS_tag2 + \"\\tBIO_plus_2=\" + next_BIO_tag2 + \"\\tstemmed_word_plus_2=\" \\\n",
    "                            + stemmed_next_token2 + \"\\t\" \n",
    "#                     + next_token_num2 + \"\\t\" + next_sentence_num2 + \"\\t\" + next_role_label2 + next_class_of_predicate2 \n",
    "                               \n",
    "    \n",
    "    else:\n",
    "        next_info1 = \"Ending Sentence\\t\"\n",
    "\n",
    "\n",
    "#     if len(t) > 5:\n",
    "#             role_label = t[5]\n",
    "#             if role_label == \"ARG1\":\n",
    "#                 arg1_index = int(t[3])\n",
    "\n",
    "    sentence.append(token)\n",
    "\n",
    "\n",
    "    line = \"word=\" + token + \"\\tPOS=\" + POS_tag + \"\\tBIO=\" + BIO_tag + \"\\tstemmed_word=\" + stemmed_token + \\\n",
    "                \"\\t\" + prev_info1 + prev_info2 + next_info1 + next_info2 + str(arg1_label) + \"\\n\"\n",
    "    f.write(line)\n",
    "\n",
    "f.close()\n",
    "\n",
    "file = open(test_file,\"r\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "f = open(\"test.feature\", \"w\")\n",
    "test_features = []\n",
    "for line in lines:\n",
    "    t = line.split()\n",
    "    test_features.append(t)\n",
    "\n",
    "sentence = []\n",
    "# text_lines = []\n",
    "# arg1_index = 0\n",
    "for i in range(len(test_features)):\n",
    "    t = test_features[i]\n",
    "    if len(t) == 0:\n",
    "        \n",
    "\n",
    "\n",
    "        line = \"\\n\"\n",
    "        f.write(line)\n",
    "        sentence.clear()\n",
    "#         text_lines.clear()\n",
    "        # print(\"end\")\n",
    "        continue\n",
    "    token = t[0]\n",
    "    POS_tag = t[1]\n",
    "    BIO_tag = t[2]\n",
    "#     token_num = t[3]\n",
    "#     sentence_num = t[4]\n",
    "#     role_label = \"\"\n",
    "#     class_of_predicate = \"\"\n",
    "    stemmed_token = ps.stem(token)\n",
    "    \n",
    "#     if len(t) > 5:\n",
    "#         role_label = t[5]\n",
    "#         if role_label == \"PRED\":\n",
    "#             class_of_predicate = t[6]\n",
    "       \n",
    "    prev_info1 = \"\"\n",
    "    prev_info2 = \"\"\n",
    "    if test_features[i-1]:\n",
    "        prev_t1 = test_features[i-1]\n",
    "        prev_token1 = prev_t1[0]\n",
    "        prev_POS_tag1 = prev_t1[1]\n",
    "        prev_BIO_tag1 = prev_t1[2]\n",
    "#         prev_token_num1 = prev_t1[3]\n",
    "#         prev_sentence_num1 = prev_t1[4]\n",
    "        stemmed_prev_token1 = ps.stem(prev_token1)\n",
    "#         prev_role_label1 = \"\"\n",
    "#         prev_class_of_predicate1 = \"\"\n",
    "#         if len(prev_t1) > 5:\n",
    "#             prev_role_label1 = prev_t1[5]\n",
    "#             if prev_role_label1 == \"PRED\":\n",
    "#                 prev_class_of_predicate1 = prev_t1[6]\n",
    "\n",
    "        prev_info1 = \"word_back_1=\" + prev_token1 + \"\\tPOS_back_1=\" + prev_POS_tag1 + \"\\tBIO_back_1=\" + prev_BIO_tag1 + \"\\tstemmed_word_back_1=\" \\\n",
    "                        + stemmed_prev_token1 + \"\\t\"\n",
    "#                 + prev_token_num1 + \"\\t\" + prev_sentence_num1  + \"\\t\"\n",
    "\n",
    "        if test_features[i-2]:\n",
    "            prev_t2 = test_features[i-2]\n",
    "            prev_token2 = prev_t2[0]\n",
    "            prev_POS_tag2 = prev_t2[1]\n",
    "            prev_BIO_tag2 = prev_t2[2]\n",
    "            prev_token_num2 = prev_t2[3]\n",
    "            prev_sentence_num2 = prev_t2[4]\n",
    "            stemmed_prev_token2 = ps.stem(prev_token2)\n",
    "            prev_role_label2 = \"\"\n",
    "            prev_class_of_predicate2 = \"\"\n",
    "            if len(prev_t2) > 5:\n",
    "                prev_role_label2 = prev_t2[5]\n",
    "                if prev_role_label2 == \"PRED\":\n",
    "                    prev_class_of_predicate2 = prev_t2[6]\n",
    "\n",
    "            prev_info2 = \"word_back_2=\" + prev_token2 + \"\\tPOS_back_2=\" + prev_POS_tag2 + \"\\tBIO_back_2=\" + prev_BIO_tag2 + \"\\tstemmed_word_back_2=\" \\\n",
    "                        + stemmed_prev_token2 + \"\\t\" \n",
    "#                     + prev_token_num2 + \"\\t\" + prev_sentence_num2 + \"\\t\"\n",
    "    \n",
    "    else:\n",
    "        prev_info1 = \"Beginning Sentence\\t\"\n",
    "\n",
    "    next_info1 = \"\"\n",
    "    next_info2 = \"\"\n",
    "    if test_features[i+1]:\n",
    "        next_t1 = test_features[i+1]\n",
    "        next_token1 = next_t1[0]\n",
    "        next_POS_tag1 = next_t1[1]\n",
    "        next_BIO_tag1 = next_t1[2]\n",
    "        stemmed_next_token1 = ps.stem(next_token1)\n",
    "        next_token_num1 = next_t1[3]\n",
    "        next_sentence_num1 = next_t1[4]\n",
    "        next_role_label1 = \"\"\n",
    "        next_class_of_predicate1 = \"\"\n",
    "        if len(next_t1) > 5:\n",
    "                next_role_label1 = next_t1[5]\n",
    "                if next_role_label1 == \"PRED\":\n",
    "                    next_class_of_predicate1 = next_t1[6]\n",
    "        next_info1 = \"word_plus_1=\" + next_token1 + \"\\tPOS_plus_1=\" + next_POS_tag1 + \"\\tBIO_plus_1=\" + next_BIO_tag1 + \"\\tstemmed_word_plus_1=\" \\\n",
    "                            + stemmed_next_token1 + \"\\t\" \n",
    "#                     + next_token_num1 + \"\\t\" + next_sentence_num1 + \"\\t\"\n",
    "\n",
    "        if test_features[i+2]:\n",
    "            next_t2 = test_features[i+2]\n",
    "            next_token2 = next_t2[0]\n",
    "            next_POS_tag2 = next_t2[1]\n",
    "            next_BIO_tag2 = next_t2[2]\n",
    "            stemmed_next_token2 = ps.stem(next_token2)\n",
    "            next_token_num2 = next_t2[3]\n",
    "            next_sentence_num2 = next_t2[4]\n",
    "            next_role_label2 = \"\"\n",
    "            next_class_of_predicate2 = \"\"\n",
    "            if len(next_t2) > 5:\n",
    "                    next_role_label2 = next_t2[5]\n",
    "                    if next_role_label2 == \"PRED\":\n",
    "                        next_class_of_predicate2 = next_t2[6]\n",
    "            next_info2 = \"word_plus_2=\" + next_token2 + \"\\tPOS_plus_2=\" + next_POS_tag2 + \"\\tBIO_plus_2=\" + next_BIO_tag2 + \"\\tstemmed_word_plus_2=\" \\\n",
    "                            + stemmed_next_token2 + \"\\t\" \n",
    "#                     + next_token_num2 + \"\\t\" + next_sentence_num2 + \"\\t\"\n",
    "\n",
    "    \n",
    "    else:\n",
    "        next_info1 = \"Ending Sentence\\t\"\n",
    "\n",
    "\n",
    "#     if len(t) > 5:\n",
    "#             role_label = t[5]\n",
    "#             if role_label == \"ARG1\":\n",
    "#                 arg1_index = int(t[3])\n",
    "\n",
    "#     sentence.append(token)\n",
    "\n",
    "\n",
    "    line = \"word=\" + token + \"\\tPOS=\" + POS_tag + \"\\tBIO=\" + BIO_tag + \"\\tstemmed_word=\" + stemmed_token + \\\n",
    "                \"\\t\" + prev_info1 + prev_info2 + next_info1 + next_info2  + \"\\n\"\n",
    "    f.write(line)\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48bcd4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!javac -cp maxent-3.0.0.jar:trove.jar *.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "208d42e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing events using cutoff of 4\n",
      "\n",
      "\tComputing event counts...  done. 61131 events\n",
      "\tIndexing...  done.\n",
      "Sorting and merging events... done. Reduced 61131 events to 46742.\n",
      "Done indexing.\n",
      "Incorporating indexed data for training...  \n",
      "done.\n",
      "\tNumber of Event Tokens: 46742\n",
      "\t    Number of Outcomes: 2\n",
      "\t  Number of Predicates: 15585\n",
      "...done.\n",
      "Computing model parameters...\n",
      "Performing 100 iterations.\n",
      "  1:  .. loglikelihood=-42372.780294768825\t0.963602754739821\n",
      "  2:  .. loglikelihood=-8789.667687066267\t0.963602754739821\n",
      "  3:  .. loglikelihood=-6817.103560205527\t0.9644370286761218\n",
      "  4:  .. loglikelihood=-5810.670890400104\t0.9676923328589423\n",
      "  5:  .. loglikelihood=-5173.389255096895\t0.9702278712927974\n",
      "  6:  .. loglikelihood=-4737.35144656101\t0.9719127774778754\n",
      "  7:  .. loglikelihood=-4417.671154792107\t0.9737612667877181\n",
      "  8:  .. loglikelihood=-4170.8446386878795\t0.9749390652860251\n",
      "  9:  .. loglikelihood=-3973.0441816603598\t0.9756915476599434\n",
      " 10:  .. loglikelihood=-3810.1326511316815\t0.9763949550964323\n",
      " 11:  .. loglikelihood=-3673.1085185883435\t0.9773273789075919\n",
      " 12:  .. loglikelihood=-3555.9074511749527\t0.9781125779064632\n",
      " 13:  .. loglikelihood=-3454.26288566731\t0.9787341937805696\n",
      " 14:  .. loglikelihood=-3365.0716366566967\t0.9791922265299112\n",
      " 15:  .. loglikelihood=-3286.016976367347\t0.9798465590289706\n",
      " 16:  .. loglikelihood=-3215.332159613578\t0.9804190999656476\n",
      " 17:  .. loglikelihood=-3151.6455031319315\t0.9808607744025126\n",
      " 18:  .. loglikelihood=-3093.8754031247854\t0.9811715823395658\n",
      " 19:  .. loglikelihood=-3041.1572533792396\t0.9817114066512899\n",
      " 20:  .. loglikelihood=-2992.7914260178522\t0.9819731396509136\n",
      " 21:  .. loglikelihood=-2948.205530739721\t0.9822839475879668\n",
      " 22:  .. loglikelihood=-2906.926569977618\t0.9825620389000671\n",
      " 23:  .. loglikelihood=-2868.5600869220616\t0.9827092637123555\n",
      " 24:  .. loglikelihood=-2832.7743417429524\t0.9828892051495968\n",
      " 25:  .. loglikelihood=-2799.2881602338066\t0.9832654463365559\n",
      " 26:  .. loglikelihood=-2767.861502203258\t0.9834290294613208\n",
      " 27:  .. loglikelihood=-2738.2880689838667\t0.983739837398374\n",
      " 28:  .. loglikelihood=-2710.389456523913\t0.9838870622106624\n",
      " 29:  .. loglikelihood=-2684.010491519031\t0.9839852120855213\n",
      " 30:  .. loglikelihood=-2659.015481248557\t0.9841324368978096\n",
      " 31:  .. loglikelihood=-2635.285175040598\t0.9843450949600039\n",
      " 32:  .. loglikelihood=-2612.714284406829\t0.9844596031473393\n",
      " 33:  .. loglikelihood=-2591.2094451205885\t0.9845250363972453\n",
      " 34:  .. loglikelihood=-2570.6875314730714\t0.9847376944594396\n",
      " 35:  .. loglikelihood=-2551.074253164948\t0.9849012775842044\n",
      " 36:  .. loglikelihood=-2532.302980568295\t0.9849994274590633\n",
      " 37:  .. loglikelihood=-2514.313755716844\t0.9849994274590633\n",
      " 38:  .. loglikelihood=-2497.052455286904\t0.9851302939588752\n",
      " 39:  .. loglikelihood=-2480.470078697905\t0.9851957272087811\n",
      " 40:  .. loglikelihood=-2464.522139793397\t0.9852775187711635\n",
      " 41:  .. loglikelihood=-2449.16814472962\t0.9853920269584989\n",
      " 42:  .. loglikelihood=-2434.371141976736\t0.9855065351458343\n",
      " 43:  .. loglikelihood=-2420.0973329318404\t0.9855883267082168\n",
      " 44:  .. loglikelihood=-2406.315733709835\t0.9856537599581228\n",
      " 45:  .. loglikelihood=-2392.997880332934\t0.9857355515205052\n",
      " 46:  .. loglikelihood=-2380.1175708744154\t0.9858173430828876\n",
      " 47:  .. loglikelihood=-2367.6506391916246\t0.9858827763327935\n",
      " 48:  .. loglikelihood=-2355.574755764274\t0.9859482095826995\n",
      " 49:  .. loglikelihood=-2343.869251872928\t0.9859972845201289\n",
      " 50:  .. loglikelihood=-2332.514963944736\t0.9860463594575584\n",
      " 51:  .. loglikelihood=-2321.4940953820205\t0.9860463594575584\n",
      " 52:  .. loglikelihood=-2310.790093594232\t0.9862099425823232\n",
      " 53:  .. loglikelihood=-2300.387540290835\t0.9862263008947997\n",
      " 54:  .. loglikelihood=-2290.272053374584\t0.9862426592072762\n",
      " 55:  .. loglikelihood=-2280.430199010406\t0.9863080924571821\n",
      " 56:  .. loglikelihood=-2270.8494126443406\t0.9863408090821351\n",
      " 57:  .. loglikelihood=-2261.5179279143094\t0.9864226006445175\n",
      " 58:  .. loglikelihood=-2252.424712537155\t0.9864716755819469\n",
      " 59:  .. loglikelihood=-2243.5594103767194\t0.9865698254568058\n",
      " 60:  .. loglikelihood=-2234.9122890011754\t0.9866352587067119\n",
      " 61:  .. loglikelihood=-2226.474192125319\t0.9866516170191884\n",
      " 62:  .. loglikelihood=-2218.236496409875\t0.9866516170191884\n",
      " 63:  .. loglikelihood=-2210.191072153947\t0.9866843336441413\n",
      " 64:  .. loglikelihood=-2202.330247473359\t0.9867334085815708\n",
      " 65:  .. loglikelihood=-2194.6467756057787\t0.9867661252065237\n",
      " 66:  .. loglikelihood=-2187.133805025596\t0.9867824835190002\n",
      " 67:  .. loglikelihood=-2179.7848520881653\t0.9868642750813826\n",
      " 68:  .. loglikelihood=-2172.593775954271\t0.9868642750813826\n",
      " 69:  .. loglikelihood=-2165.554755574326\t0.9868806333938591\n",
      " 70:  .. loglikelihood=-2158.662268535194\t0.986978783268718\n",
      " 71:  .. loglikelihood=-2151.911071594386\t0.986978783268718\n",
      " 72:  .. loglikelihood=-2145.29618274517\t0.9870278582061475\n",
      " 73:  .. loglikelihood=-2138.812864672154\t0.987044216518624\n",
      " 74:  .. loglikelihood=-2132.4566094717875\t0.987044216518624\n",
      " 75:  .. loglikelihood=-2126.223124525407\t0.9870605748311004\n",
      " 76:  .. loglikelihood=-2120.108319422951\t0.9871096497685299\n",
      " 77:  .. loglikelihood=-2114.1082938466493\t0.9871260080810064\n",
      " 78:  .. loglikelihood=-2108.2193263321565\t0.9871260080810064\n",
      " 79:  .. loglikelihood=-2102.4378638330136\t0.9871260080810064\n",
      " 80:  .. loglikelihood=-2096.7605120214394\t0.9871750830184358\n",
      " 81:  .. loglikelihood=-2091.184026264573\t0.9872077996433888\n",
      " 82:  .. loglikelihood=-2085.705303221358\t0.9872405162683418\n",
      " 83:  .. loglikelihood=-2080.3213730099824\t0.9872405162683418\n",
      " 84:  .. loglikelihood=-2075.029391900567\t0.9872568745808182\n",
      " 85:  .. loglikelihood=-2069.8266354921648\t0.9872568745808182\n",
      " 86:  .. loglikelihood=-2064.710492335815\t0.9872895912057712\n",
      " 87:  .. loglikelihood=-2059.678457970467\t0.9873386661432006\n",
      " 88:  .. loglikelihood=-2054.728129339493\t0.9873386661432006\n",
      " 89:  .. loglikelihood=-2049.8571995600714\t0.9873550244556771\n",
      " 90:  .. loglikelihood=-2045.063453018894\t0.9873713827681536\n",
      " 91:  .. loglikelihood=-2040.3447607705316\t0.9873713827681536\n",
      " 92:  .. loglikelihood=-2035.6990762164908\t0.9874204577055831\n",
      " 93:  .. loglikelihood=-2031.1244310449545\t0.9874368160180595\n",
      " 94:  .. loglikelihood=-2026.6189314127357\t0.9874368160180595\n",
      " 95:  .. loglikelihood=-2022.180754352576\t0.9874695326430125\n",
      " 96:  .. loglikelihood=-2017.8081443900005\t0.987485890955489\n",
      " 97:  .. loglikelihood=-2013.4994103557642\t0.987485890955489\n",
      " 98:  .. loglikelihood=-2009.2529223803197\t0.987485890955489\n",
      " 99:  .. loglikelihood=-2005.0671090582814\t0.987518607580442\n",
      "100:  .. loglikelihood=-2000.940454771553\t0.9875676825178714\n"
     ]
    }
   ],
   "source": [
    "!java -cp .:maxent-3.0.0.jar:trove.jar MEtrain training.feature model.chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3bd61ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -cp .:maxent-3.0.0.jar:trove.jar MEtag test.feature model.chunk response.chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "66293cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"response.chunk\"\n",
    "output_file = \"output.txt\"\n",
    "file = open(input_file,\"r\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "input_features = []\n",
    "for line in lines:\n",
    "    t = line.split()\n",
    "#     if len(t) == 0:\n",
    "#         input_features.pop()\n",
    "#         input_features.pop()\n",
    "#         input_features.pop()\n",
    "#         input_features.pop()\n",
    "#         input_features.pop()\n",
    "    input_features.append(t)\n",
    "\n",
    "f = open(output_file, \"w\")\n",
    "for i in range(len(input_features)):\n",
    "    t = input_features[i]\n",
    "    # print(t)\n",
    "    if len(t) == 0:\n",
    "        line = \"\\n\"\n",
    "        f.write(line)\n",
    "        continue\n",
    "    token = t[0]\n",
    "    arg1 = t[1]\n",
    "#     if len(t) > 1:\n",
    "#         arg1 = t[1]\n",
    "    line = token[5:]\n",
    "    if arg1 == \"1\":\n",
    "        line += \"\\tARG1\\n\"\n",
    "    else:\n",
    "        line += \"\\n\"\n",
    "\n",
    "    f.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4b80ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arg_scorer import score_file_with_NNP_adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "10bf60e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System [9, 40, 62, 103, 116, 136, 154, 190, 195, 239, 245, 276, 320, 355, 380, 397, 414, 421, 438, 445, 462, 469, 501, 523, 557, 581, 660, 722, 756, 784, 834, 870, 932, 956, 988, 1019, 1142, 1187, 1232, 1277, 1298, 1313, 1328, 1432, 1457, 1482, 1490, 1525, 1547, 1570, 1592, 1615, 1642, 1664, 1693, 1739, 1744, 1752, 1810, 1839, 1863, 1941, 1947, 2099, 2128, 2161, 2221, 2414, 2433, 2502, 2527, 2550, 2563, 2573, 2586, 2597, 2611, 2633, 2645, 2661, 2674, 2690, 2703, 2719, 2732, 2748, 2762, 2780, 2809, 2838, 2893, 2932, 2959, 2966, 2989, 3040, 3052, 3062]\n",
      "Answers [9, 62, 78, 103, 116, 154, 169, 195, 229, 257, 288, 320, 355, 380, 421, 438, 462, 501, 523, 557, 581, 612, 637, 660, 686, 722, 742, 756, 784, 834, 870, 904, 909, 932, 956, 976, 1013, 1057, 1096, 1142, 1168, 1169, 1170, 1171, 1223, 1224, 1225, 1277, 1298, 1313, 1328, 1345, 1352, 1353, 1354, 1384, 1385, 1386, 1432, 1457, 1482, 1490, 1547, 1592, 1615, 1642, 1664, 1675, 1693, 1739, 1783, 1810, 1828, 1852, 1863, 1891, 1947, 1972, 2003, 2023, 2063, 2099, 2128, 2161, 2221, 2254, 2279, 2302, 2333, 2400, 2401, 2414, 2458, 2487, 2502, 2527, 2563, 2573, 2597, 2611, 2633, 2645, 2690, 2719, 2732, 2762, 2780, 2809, 2838, 2893, 2932, 2959, 2989, 3052, 3086, 3116, 3136, 3165, 3248, 3262, 3292, 3321, 3356, 3376, 3403, 3427, 3457, 3479, 3513, 3515, 3531, 3566, 3591, 3647, 3661, 3673, 3703, 3704, 3705, 3734, 3760, 3761, 3762, 3780, 3793, 3794, 3813, 3814, 3815, 3816, 3817, 3848, 3849, 3850, 3851, 3864, 3865, 3909, 3923, 3974, 4022, 4054, 4072, 4094, 4130, 4160, 4206, 4237, 4275, 4308, 4330, 4340, 4371, 4425]\n",
      "correct 68\n",
      "lengths 98 174\n",
      "precision =  69.39\n",
      "recall =  39.08\n",
      "f-measure =  50.0\n"
     ]
    }
   ],
   "source": [
    "score_file_with_NNP_adjustment('%_nombank.clean.test', 'output.txt', 'ARG1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af12b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
